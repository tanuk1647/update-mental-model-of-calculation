{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算するって何だろう（for ディープラーニング）\n",
    "\n",
    "## はじめに\n",
    "ディープラーニングを学習する過程で、計算についてのメンタルモデル——計算とはこういうものだというイメージ——が拡張されるのを感じました。\n",
    "\n",
    "本記事は、その新しいメンタルモデルを共有するために書かれました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算とは\n",
    "世界で一番易しい計算、といえば以下でしょう。\n",
    "\n",
    "$$\n",
    "1 + 1 =\\,?\n",
    "$$\n",
    "\n",
    "この易しい計算を難しく分解すると以下となります。\n",
    "\n",
    "- 構造：$y = f(x_0, x_1) = x_0 + x_1$\n",
    "- 要素：$x_0 = 1, x_1 = 1, y = unknown$\n",
    "\n",
    "そして、計算とは、構造と要素（一部が未知数）が与えられた状態で、**構造に「フィット」する要素の組を求めること**と定義できます。\n",
    "\n",
    "上述の計算は、順方向——左辺から右辺——へと、一度だけ行われます。\n",
    "\n",
    "世の中の大多数の人々にとって、計算とは、順方向へと一度だけ行われるものでしょう。しかし、そうではない計算を考えることもできます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 双方向に、何度でも（１）\n",
    "例えば、以下のような計算を考えます。\n",
    "\n",
    "$$\n",
    "?\\,+\\,?\\,= 2\n",
    "$$\n",
    "\n",
    "- 構造：$y = f(x_0, x_1) = x_0 + x_1$\n",
    "- 要素：$x_0 = unknown, x_1 = unknown, y = 2$\n",
    "\n",
    "構造は先ほどの「世界で一番易しい計算」と同じですが、要素が2点異なります。\n",
    "\n",
    "- 左辺が未知数であること\n",
    "- 要素の組が一意に定まりそうにないこと\n",
    "\n",
    "以上の計算に対し、以下の3つのアクションを組み合わせて、計算を行うものとします。\n",
    "\n",
    "- 初期値の更新\n",
    "- 順方向への計算\n",
    "- 逆方向へのフィードバック\n",
    "\n",
    "例えば、こんなアプローチが考えられるでしょう。\n",
    "\n",
    "1. 未知数をランダムに初期化する\n",
    "1. 順方向への計算を行う\n",
    "1. 計算結果と真の値の誤差が十分小さければ終了する\n",
    "1. 逆方向へとフィードバックする（未知数をランダムに初期化する）\n",
    "1. 2に戻る\n",
    "\n",
    "計算は、双方向に何度も行われます。誤差は、以下の誤差関数eで表すものとします。\n",
    "\n",
    "$$\n",
    "e = \\frac{1}{2}\\{y - (x_0 + x_1)\\}^2\n",
    "$$\n",
    "\n",
    "二乗しているのは正負の違いを吸収するため、二分の一倍しているのは、のちの都合上です。\n",
    "\n",
    "以下がコードの例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463回の計算で、x0が-1.724、x1が3.728と判明。計算結果は2.004です。\n",
      "1回の計算で、x0が4.247、x1が-2.259と判明。計算結果は1.987です。\n",
      "99回の計算で、x0が0.465、x1が1.529と判明。計算結果は1.993です。\n",
      "230回の計算で、x0が-1.126、x1が3.137と判明。計算結果は2.010です。\n",
      "983回の計算で、x0が0.227、x1が1.784と判明。計算結果は2.011です。\n",
      "143回の計算で、x0が2.904、x1が-0.902と判明。計算結果は2.003です。\n",
      "22回の計算で、x0が1.947、x1が0.066と判明。計算結果は2.013です。\n",
      "112回の計算で、x0が2.865、x1が-0.872と判明。計算結果は1.992です。\n",
      "1338回の計算で、x0が-2.327、x1が4.338と判明。計算結果は2.011です。\n",
      "83回の計算で、x0が3.545、x1が-1.535と判明。計算結果は2.010です。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randint():\n",
    "    return random.random() * 10 - 5\n",
    "\n",
    "def calc():\n",
    "    x0, x1, y = randint(), randint(), 2\n",
    "    delta = 0.0001\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        # forward\n",
    "        out = x0 + x1\n",
    "        \n",
    "        # backward\n",
    "        error = 0.5 * (y - out) ** 2\n",
    "        if error < delta: break\n",
    "        dout = randint(), randint()\n",
    "        x0, x1 = dout[0], dout[1]\n",
    "    message = '{}回の計算で、x0が{:.3f}、x1が{:.3f}と判明。計算結果は{:.3f}です。'\n",
    "    print(message.format(counter, x0, x1, x0 + x1))\n",
    "\n",
    "for _ in range(10): calc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確かに（近似値レベルではありますが）計算できており、アプローチが有効であることがわかります。\n",
    "\n",
    "しかし、以下の点を改善できないものでしょうか。\n",
    "\n",
    "- 計算回数が多い\n",
    "- 計算回数がバラつく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 双方向に、何度でも（２）\n",
    "結論的には、以下アプローチにより、計算回数を少なく、かつ、安定させることができます。\n",
    "\n",
    "1. 未知数をランダムに初期化する\n",
    "1. 順方向への計算を行う\n",
    "1. 計算結果と真の値の誤差が十分小さければ終了する\n",
    "1. 逆方向へとフィードバックする（要素を誤差関数の偏微分結果で更新する）\n",
    "1. 2に戻る\n",
    "\n",
    "微分が出てくるのは、それが誤差を小さくするための合理的な方法だからです。というのも、誤差関数eを微分することで、勾配がわかるので、要素を増やせば良いのか、減らせば良いのかわかるのです。\n",
    "\n",
    "数式は以下のとおり。\n",
    "\n",
    "$$\n",
    "e = \\frac{1}{2}\\{y - (x_0 + x_1)\\}^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial x_0} = \\frac{\\partial e}{\\partial x_1} = -y + x_0 + x_1\n",
    "$$\n",
    "\n",
    "以下がコードの例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19回の計算で、x0が-1.544、x1が3.556と判明。計算結果は2.012です。\n",
      "30回の計算で、x0が0.149、x1が1.838と判明。計算結果は1.988です。\n",
      "30回の計算で、x0が2.167、x1が-0.179と判明。計算結果は1.987です。\n",
      "17回の計算で、x0が2.103、x1が-0.089と判明。計算結果は2.014です。\n",
      "24回の計算で、x0が3.955、x1が-1.968と判明。計算結果は1.988です。\n",
      "11回の計算で、x0が4.346、x1が-2.335と判明。計算結果は2.012です。\n",
      "29回の計算で、x0が-0.809、x1が2.797と判明。計算結果は1.988です。\n",
      "21回の計算で、x0が1.667、x1が0.347と判明。計算結果は2.014です。\n",
      "14回の計算で、x0が4.051、x1が-2.065と判明。計算結果は1.986です。\n",
      "28回の計算で、x0が0.947、x1が1.039と判明。計算結果は1.986です。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randint():\n",
    "    return random.random() * 10 - 5\n",
    "\n",
    "def calc():\n",
    "    x0, x1, y = randint(), randint(), 2\n",
    "    alpha, delta = 0.1, 0.0001\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        # forward\n",
    "        out = x0 + x1\n",
    "        \n",
    "        # backward\n",
    "        error = 0.5 * (y - out) ** 2\n",
    "        if error < delta: break\n",
    "        dout = (-y + x0 + x1, -y + x0 + x1)\n",
    "        x0, x1 = x0 - alpha * dout[0], x1 - alpha * dout[1]\n",
    "    message = '{}回の計算で、x0が{:.3f}、x1が{:.3f}と判明。計算結果は{:.3f}です。'\n",
    "    print(message.format(counter, x0, x1, x0 + x1))\n",
    "\n",
    "for _ in range(10): calc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算回数が少なく、かつ、安定していることがわかります。\n",
    "\n",
    "乗算についても、見てみましょう。数式は以下のとおり。\n",
    "\n",
    "$$\n",
    "e = \\frac{1}{2}\\{y - (x_0 \\cdot x_1)\\}^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial x_0} = -y \\cdot x_1 + {x_1}^2 \\cdot x_0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial x_1} = -y \\cdot x_0 + {x_0}^2 \\cdot x_1\n",
    "$$\n",
    "\n",
    "以下がコードの例です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19回の計算で、x0が1.504、x1が1.320と判明。計算結果は1.986です。\n",
      "21回の計算で、x0が1.338、x1が1.488と判明。計算結果は1.990です。\n",
      "8回の計算で、x0が2.364、x1が0.841と判明。計算結果は1.988です。\n",
      "17回の計算で、x0が-1.661、x1が-1.198と判明。計算結果は1.991です。\n",
      "18回の計算で、x0が1.238、x1が1.607と判明。計算結果は1.989です。\n",
      "17回の計算で、x0が1.266、x1が1.571と判明。計算結果は1.988です。\n",
      "13回の計算で、x0が1.468、x1が1.357と判明。計算結果は1.991です。\n",
      "10回の計算で、x0が1.486、x1が1.353と判明。計算結果は2.010です。\n",
      "19回の計算で、x0が1.305、x1が1.524と判明。計算結果は1.989です。\n",
      "3回の計算で、x0が0.616、x1が3.265と判明。計算結果は2.010です。\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randint():\n",
    "    return random.random() * 10 - 5\n",
    "\n",
    "def calc():\n",
    "    x0, x1, y = randint(), randint(), 2\n",
    "    alpha, delta = 0.1, 0.0001\n",
    "    counter = 0\n",
    "    while True:\n",
    "        counter += 1\n",
    "        # forward\n",
    "        out = x0 * x1\n",
    "        \n",
    "        # backward\n",
    "        error = 0.5 * (y - out) ** 2\n",
    "        if error < delta: break\n",
    "        dout = (-y * x1 + (x1 ** 2) * x0, -y * x0 + (x0 ** 2) * x1)\n",
    "        x0, x1 = x0 - alpha * dout[0], x1 - alpha * dout[1]\n",
    "    message = '{}回の計算で、x0が{:.3f}、x1が{:.3f}と判明。計算結果は{:.3f}です。'\n",
    "    print(message.format(counter, x0, x1, x0 * x1))\n",
    "\n",
    "for _ in range(10): calc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加算と同様、計算回数が少なく、かつ、安定していることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ディープラーニング\n",
    "ディープラーニングで、レイヤーを重ねてネットワークを定義するとき、計算の構造が定まります。また、計算の要素のうち、左辺の入力値と、右辺の出力値も定まります（教師あり学習）。しかし、その他の要素（重みやバイアス）については、未知数のままです。これをランダムに初期化し、双方向に何度も計算を行い、構造にフィットする要素の組を求める、ということが行われます。構造にフィットする要素の組は複数あるので、そのうちの1つを求めているイメージです。\n",
    "\n",
    "[Keras Blog](https://blog.keras.io/building-autoencoders-in-keras.html)を参考に、-100〜100の実数xを入力、および、出力するオートエンコーダ（自己符号化器）を実装してみます。xをエンコードし、すぐにデコードします。活性化関数は用いません。数式で表すと、以下の構造にフィットする重み（Weight）とバイアス（Bias）の組を求める問題です。とても単純な例です。\n",
    "\n",
    "$$\n",
    "w_{dec} \\cdot (w_{enc} \\cdot x + b_{enc}) + b_{dec} = x\n",
    "$$\n",
    "\n",
    "以下に、コードと実行結果を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(18000,)\n",
      "(2000,)\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 2567.6244 - val_loss: 1672.4072\n",
      "Epoch 2/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 1124.5867 - val_loss: 711.7286\n",
      "Epoch 3/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 471.2158 - val_loss: 291.2063\n",
      "Epoch 4/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 189.0829 - val_loss: 113.7114\n",
      "Epoch 5/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 72.1011 - val_loss: 41.8325\n",
      "Epoch 6/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 25.8259 - val_loss: 14.3671\n",
      "Epoch 7/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 8.6155 - val_loss: 4.5759\n",
      "Epoch 8/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 2.6525 - val_loss: 1.3376\n",
      "Epoch 9/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 0.7479 - val_loss: 0.3557\n",
      "Epoch 10/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 0.1915 - val_loss: 0.0856\n",
      "Epoch 11/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 0.0444 - val_loss: 0.0186\n",
      "Epoch 12/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 0.0093 - val_loss: 0.0036\n",
      "Epoch 13/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 0.0017 - val_loss: 6.3907e-04\n",
      "Epoch 14/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 2.9552e-04 - val_loss: 1.0065e-04\n",
      "Epoch 15/30\n",
      "18000/18000 [==============================] - 0s 3us/step - loss: 4.5159e-05 - val_loss: 1.4668e-05\n",
      "Epoch 16/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 6.5585e-06 - val_loss: 2.1619e-06\n",
      "Epoch 17/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 1.0537e-06 - val_loss: 4.3299e-07\n",
      "Epoch 18/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 2.7576e-07 - val_loss: 1.7147e-07\n",
      "Epoch 19/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 1.3067e-07 - val_loss: 9.9948e-08\n",
      "Epoch 20/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 7.9055e-08 - val_loss: 6.3693e-08\n",
      "Epoch 21/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 5.3266e-08 - val_loss: 4.6208e-08\n",
      "Epoch 22/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 4.0863e-08 - val_loss: 3.7159e-08\n",
      "Epoch 23/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 3.2356e-08 - val_loss: 2.9349e-08\n",
      "Epoch 24/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 2.7477e-08 - val_loss: 2.6515e-08\n",
      "Epoch 25/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 2.4333e-08 - val_loss: 2.2376e-08\n",
      "Epoch 26/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 2.1628e-08 - val_loss: 2.1058e-08\n",
      "Epoch 27/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 1.9522e-08 - val_loss: 1.8806e-08\n",
      "Epoch 28/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 1.7792e-08 - val_loss: 1.7726e-08\n",
      "Epoch 29/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 1.7062e-08 - val_loss: 1.6747e-08\n",
      "Epoch 30/30\n",
      "18000/18000 [==============================] - 0s 4us/step - loss: 1.5183e-08 - val_loss: 1.4879e-08\n",
      "-------------\n",
      "32.99000000006805\n",
      "-35.798763\n",
      "32.990067\n",
      "-------------\n",
      "-33.85999999996616\n",
      "36.74323\n",
      "-33.860077\n",
      "-------------\n",
      "-95.41999999999766\n",
      "103.54479\n",
      "-95.4202\n",
      "-------------\n",
      "-86.44999999999307\n",
      "93.811035\n",
      "-86.45018\n",
      "-------------\n",
      "-27.51999999996292\n",
      "29.863403\n",
      "-27.52006\n",
      "-------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[-1.0851457]], dtype=float32),\n",
       " array([0.00019342], dtype=float32),\n",
       " array([[-0.92153716]], dtype=float32),\n",
       " array([0.00017642], dtype=float32)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 1\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(1,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation=None)(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(1, activation=None)(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "print(autoencoder.summary())\n",
    "autoencoder.compile(optimizer='nadam', loss='mean_squared_error')\n",
    "\n",
    "def load_data():\n",
    "    data = np.arange(-100, 100, 0.01)\n",
    "    np.random.shuffle(data)\n",
    "    border = len(data) * 9 // 10\n",
    "    return data[: border], data[border :]\n",
    "\n",
    "import numpy as np\n",
    "x_train, x_test = load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=30,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))\n",
    "\n",
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "n = 5  # how many digits we will display\n",
    "\n",
    "print('-------------')\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    print(x_test[i])\n",
    "    \n",
    "    # display encoded representation\n",
    "    print(encoded_imgs[i][0])\n",
    "    \n",
    "    # display reconstruction\n",
    "    print(decoded_imgs[i][0])\n",
    "    \n",
    "    print('-------------')\n",
    "    \n",
    "autoencoder.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5回実行し、得られた重みとバイアスは、以下のとおりです。\n",
    "\n",
    "|$w_{enc}$|$b_{enc}$|$w_{dec}$|$b_{dec}$|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|-0.52114826|0.0008159|-1.9188374|0.0015671|\n",
    "|1.6570777|0.01260246|0.6034708|-0.00760303|\n",
    "|-1.3857455|-0.01825085|-0.72163296|-0.01317422|\n",
    "|-0.6569005|-0.00015455|-1.5222994|-0.00023566|\n",
    "|-1.0851457|0.00019342|-0.92153716|0.00017642]|\n",
    "\n",
    "構造にフィットする要素の組が複数ある、ということがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "どうやら、ディープラーニングを理解するためには、計算についてのメンタルモデルを、以下のとおりに拡張する必要がありそうです。\n",
    "\n",
    "まず、大前提として、計算について、以下のとおりに定義します。\n",
    "\n",
    "- 計算とは、構造と要素（一部が未知数）が与えられた状態で、構造に「フィット」する要素の組を求めることである\n",
    "\n",
    "その上で、メンタルモデルを以下のとおりに拡張します。\n",
    "\n",
    "- 計算とは、順方向に一度だけ行われるものだ\n",
    "- 構造にフィットする要素の組は、一意に定まるものだ\n",
    "\n",
    "↓\n",
    "\n",
    "- 計算は、双方向に何度も行われることがある\n",
    "  - フィッティング途上の要素を用いて順方向に計算\n",
    "  - 逆方向へとフィードバック（要素を誤差関数の偏微分結果で更新）\n",
    "- 構造にフィットする要素の組が、複数あることもある\n",
    "\n",
    "最終的に等号が成立する点においては同じなのですが、過程が異なるのです。\n",
    "\n",
    "以上を示して、本記事を終えたいと思います。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introduction",
   "language": "python",
   "name": "introduction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
